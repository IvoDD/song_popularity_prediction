{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import zipfile\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\master.zip\n"
     ]
    }
   ],
   "source": [
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(url, filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "#dataset_tar = maybe_download('http://static.echonest.com/', 'millionsongsubset_full.tar.gz', 1994614463)\n",
    "dataset_tar = \"asdf\"\n",
    "MSongsDB_tar = maybe_download('https://github.com/tbertinmahieux/MSongsDB/archive/', 'master.zip', 17585436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MillionSongSubset already present - Skipping extraction of asdf.\n",
      "MillionSongSubset already present - Skipping extraction of .\\master.zip.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('MillionSongSubset'):\n",
    "    print('MillionSongSubset already present - Skipping extraction of %s.' % dataset_tar)\n",
    "else:\n",
    "    print('Extracting data for MillionSongSubset. This may take a while. Please wait.')\n",
    "    tar = tarfile.open(dataset_tar)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "\n",
    "if os.path.isdir('MSongsDB-master'):\n",
    "    print('MillionSongSubset already present - Skipping extraction of %s.' % MSongsDB_tar)\n",
    "else:\n",
    "    print('Extracting data for MillionSongSubset.')\n",
    "    zip_ref = zipfile.ZipFile(MSongsDB_tar, 'r')\n",
    "    zip_ref.extractall(data_root)\n",
    "    zip_ref.close()\n",
    "\n",
    "msd_path=['MillionSongDataset/B', 'MillionSongDataset/C', 'MillionSongDataset/D', 'MillionSongDataset/E', 'MillionSongDataset/F', 'MillionSongDataset/G', 'MillionSongDataset/H']\n",
    "msd_data_path=msd_path\n",
    "assert os.path.isdir(msd_path[0]),'wrong path' # sanity check\n",
    "\n",
    "msd_code_path='MSongsDB-master'\n",
    "assert os.path.isdir(msd_code_path),'wrong path' # sanity check\n",
    "\n",
    "sys.path.append( os.path.join(msd_code_path,'PythonSrc') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5_getters as GETTERS\n",
    "# the following function simply gives us a nice string for\n",
    "# a time lag in seconds\n",
    "def strtimedelta(starttime,stoptime):\n",
    "    return str(datetime.timedelta(seconds=stoptime-starttime))\n",
    "\n",
    "# we define this very useful function to iterate the files\n",
    "def apply_to_all_files(basedir,func=lambda x: x,ext='.h5'):\n",
    "    \"\"\"\n",
    "    From a base directory, go through all subdirectories,\n",
    "    find all files with the given extension, apply the\n",
    "    given function 'func' to all of them.\n",
    "    If no 'func' is passed, we do nothing except counting.\n",
    "    INPUT\n",
    "       basedir  - base directory of the dataset\n",
    "       func     - function to apply to all filenames\n",
    "       ext      - extension, .h5 by default\n",
    "    RETURN\n",
    "       number of files\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    # iterate over all files in all subdirectories\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "        # count files\n",
    "        cnt += len(files)\n",
    "        # apply function to all files\n",
    "        for f in files :\n",
    "            func(f)       \n",
    "    return cnt\n",
    "\n",
    "# we can now easily count the number of files in the dataset\n",
    "# print('number of song files:',apply_to_all_files(msd_subset_data_path))\n",
    "\n",
    "# let's now get all artist names in a set(). One nice property:\n",
    "# if we enter many times the same artist, only one will be kept.\n",
    "# all_artist_names = set()\n",
    "\n",
    "# we define the function to apply to all files\n",
    "def func_to_get_artist_name(filename):\n",
    "    \"\"\"\n",
    "    This function does 3 simple things:\n",
    "    - open the song file\n",
    "    - get artist ID and put it\n",
    "    - close the file\n",
    "    \"\"\"\n",
    "    h5 = GETTERS.open_h5_file_read(filename)\n",
    "    artist_name = GETTERS.get_artist_name(h5)\n",
    "    all_artist_names.add( artist_name )\n",
    "    h5.close()\n",
    "    \n",
    "# let's apply the previous function to all files\n",
    "# we'll also measure how long it takes\n",
    "# t1 = time.time()\n",
    "# apply_to_all_files(msd_subset_data_path,func=func_to_get_artist_name)\n",
    "# t2 = time.time()\n",
    "# print('all artist names extracted in:',strtimedelta(t1,t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some of the content of 'all_artist_names'\n",
    "# print('found',len(all_artist_names),'unique artist names')\n",
    "# for k in range(5):\n",
    "#     print(list(all_artist_names)[k].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_some_files(num, basedir, func=lambda x: x, ext='.h5'):\n",
    "    cnt = 0\n",
    "    # iterate over all files in all subdirectories\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "        # apply function to all files\n",
    "        for f in files:\n",
    "            func(f)\n",
    "            cnt += 1\n",
    "            if cnt % 10000 == 0: print(cnt)\n",
    "            if cnt >= num: break\n",
    "        if cnt >= num: break\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "getters = [ getter for getter in GETTERS.__dict__.keys() if getter[:4] == 'get_']\n",
    "getters.remove(\"get_num_songs\") # special case\n",
    "# all getters are 54\n",
    "\n",
    "cols = [getter[4:] for getter in getters]\n",
    "\n",
    "df = pd.DataFrame([], columns=cols)\n",
    "\n",
    "def func_to_get_all_info(filename):\n",
    "    h5 = GETTERS.open_h5_file_read(filename)\n",
    "    global df\n",
    "    df = df.append(pd.DataFrame([[GETTERS.__getattribute__(getter)(h5) for getter in getters]], columns=cols), ignore_index=True)\n",
    "    h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of song files: 38265 B\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "all data extracted in: 4:17:47.489009\n",
      "data saved to: B.pickle\n",
      "number of song files: 38611 C\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "all data extracted in: 1:19:37.066798\n",
      "data saved to: C.pickle\n",
      "number of song files: 38825 D\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "all data extracted in: 1:20:31.178279\n",
      "data saved to: D.pickle\n",
      "number of song files: 38466 E\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "all data extracted in: 1:19:34.585798\n",
      "data saved to: E.pickle\n",
      "number of song files: 38919 F\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "all data extracted in: 1:20:36.956103\n",
      "data saved to: F.pickle\n",
      "number of song files: 38156 G\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "all data extracted in: 1:18:27.923539\n",
      "data saved to: G.pickle\n",
      "number of song files: 38519 H\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "all data extracted in: 1:19:28.295700\n",
      "data saved to: H.pickle\n"
     ]
    }
   ],
   "source": [
    "for data_path in msd_data_path:\n",
    "    df = pd.DataFrame([], columns=cols)\n",
    "    count = apply_to_all_files(data_path)\n",
    "    print('number of song files:', count, data_path[-1])\n",
    "    t1 = time.time()\n",
    "    apply_to_some_files(count, data_path,func=func_to_get_all_info)\n",
    "    df = df.dropna(subset=['song_hotttnesss'])\n",
    "    t2 = time.time()\n",
    "    print('all data extracted in:',strtimedelta(t1,t2))\n",
    "    df.to_pickle(data_path[-1] + '.pickle')\n",
    "    print('data saved to:', data_path[-1] + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
